{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "import database as db # our script for simpler db integration\n",
    "from config import config # pull sensitive data from .ini files\n",
    "\n",
    "# set data directory\n",
    "data_path = (os.getcwd() + \"/Data/\")\n",
    "\n",
    "# connection to Postgres database\n",
    "conn = db.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create tables for database\n",
    "\n",
    "Think about schema design, primary/foreign keys, datatypes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endpoint tracker\n",
    "\n",
    "Track position ID for each endpoint in API. Enables reading from that position instead of reloading full data each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"drop table endpoint_tracker\")\n",
    "db.execute_query(conn, \"create table endpoint_tracker(endpoint_id serial primary key, endpoint_nme varchar(30), next_start int, update_dtm timestamp default current_timestamp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute_values() done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'deals', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (2, 'organizations', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (3, 'persons', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (4, 'products', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157))]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_tracker = pd.DataFrame({\n",
    "    'endpoint_nme': ['deals','organizations','persons','products'],\n",
    "    'next_start': [0, 0, 0, 0]\n",
    "})\n",
    "\n",
    "db.execute_values(conn, product_tracker, 'endpoint_tracker')\n",
    "\n",
    "# initialize db tracker\n",
    "db.execute_query(conn, \"select * from endpoint_tracker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"drop table products\")\n",
    "db.execute_query(conn, \"create table products(product_id int primary key, product_nme varchar(50), product_cde varchar(10), product_dsc varchar(100), active_ind boolean, owner_id int, owner_nme varchar(50), insert_dte date, update_dte date, price_id int, price_amt numeric(9,2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"drop table persons\")\n",
    "db.execute_query(conn, \"\"\"\n",
    "create table persons (\n",
    "    persons_id int primary key,\n",
    "    first_nme varchar(100),\n",
    "    last_nme varchar(100),\n",
    "    open_cnt int,\n",
    "    closed_cnt int,\n",
    "    emails_cnt int,\n",
    "    activities_cnt int,\n",
    "    won_cnt int,\n",
    "    lost_cnt int,\n",
    "    active_ind boolean,\n",
    "    update_dte date,\n",
    "    insert_dte date,\n",
    "    last_activity_dte date,\n",
    "    last_inmail_dte date,\n",
    "    last_outmail_dte date,\n",
    "    org_nme varchar(100),\n",
    "    owner_id int,\n",
    "    owner_nme varchar(100),\n",
    "    owner_active_ind boolean,\n",
    "    org_active_ind boolean,\n",
    "    org_id int\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract data from Pipedrive\n",
    "\n",
    "deals\n",
    "\n",
    "orgainzations\n",
    "\n",
    "persons\n",
    "\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(endpoint, start, limit):\n",
    "    \n",
    "    params = config(section='pipedrive')\n",
    "    params.update({'start':start,'limit':limit})\n",
    "    \n",
    "    response = requests.get(params['company_domain'] + endpoint, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        json_msg = response.json()\n",
    "        print('Extraction of {} data from Pipedrive complete.'.format(endpoint))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    \n",
    "    data = json_msg['data']\n",
    "    \n",
    "    if json_msg['additional_data']['pagination']['more_items_in_collection']:\n",
    "        next_start = json_msg['additional_data']['pagination']['next_start']\n",
    "    elif data is None:\n",
    "        next_start = start\n",
    "    else:\n",
    "        next_start = len(data)\n",
    "    \n",
    "    return data, next_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(data):\n",
    "    \"\"\"\n",
    "    Following rulesets should be applied for filling missing data\n",
    "    \n",
    "    ID's : -1\n",
    "    Names : blank ('')\n",
    "    Descriptions : blank ('')\n",
    "    Timestamps: 1900-01-01\n",
    "    Counts/amounts/numbers: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    id_cols = [col for col in data.columns if '_id' in col]\n",
    "    nme_cols = [col for col in data.columns if '_nme' in col]\n",
    "    dsc_cols = [col for col in data.columns if '_dsc' in col]\n",
    "    cnt_cols = [col for col in data.columns if '_cnt' in col]\n",
    "    amt_cols = [col for col in data.columns if '_amt' in col]\n",
    "    nbr_cols = [col for col in data.columns if '_nbr' in col]\n",
    "    \n",
    "    for col in (id_cols + cnt_cols + amt_cols + nbr_cols):\n",
    "        data[col].fillna(-1, inplace=True)\n",
    "    \n",
    "    for col in (nme_cols + dsc_cols):\n",
    "        data[col].fillna('', inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(endpoint, limit=500):\n",
    "    \n",
    "    if endpoint not in ['products','persons','deals','organizations']:\n",
    "        print('{} is not a valid endpoint. Please try again.'.format(endpoint))\n",
    "        return\n",
    "    \n",
    "    # find last start position\n",
    "    start_pos = db.execute_query(conn, \"select next_start from endpoint_tracker where endpoint_nme = '{endpoint}'\".format(endpoint=endpoint))[0]\n",
    "\n",
    "    # read MAX rows from most recently stored data\n",
    "    json_data, start_pos = get_data(endpoint, start=start_pos, limit=limit)\n",
    "\n",
    "    if json_data is not None:\n",
    "        # increment tracker to new start\n",
    "        db.execute_query(conn, \"update endpoint_tracker set next_start = {pos}, update_dtm = current_timestamp where endpoint_nme = '{endpoint}'\".format(pos=start_pos, endpoint=endpoint))\n",
    "\n",
    "        # convert to df\n",
    "        if endpoint == 'products':\n",
    "            data = products_to_df(json_data)\n",
    "        elif endpoint == 'persons':\n",
    "            data = persons_to_df(json_data)\n",
    "        else:\n",
    "            print('no other workflows defined right now...')\n",
    "\n",
    "        print('Transform complete.')\n",
    "        \n",
    "        # write data to database\n",
    "        db.execute_values(conn, data, endpoint)\n",
    "        \n",
    "        print('Load to database complete.')\n",
    "        \n",
    "    else:\n",
    "        print('No more {} data - tracker at position {}.'.format(endpoint, *start_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def products_to_df(json_data, dtm_to_dte=True):\n",
    "    \n",
    "    # read product data\n",
    "    products = pd.json_normalize(json_data)[['id','name','code','description','active_flag','owner_id.id','owner_id.name','add_time','update_time']]\n",
    "\n",
    "    # rename columns\n",
    "    products.rename(\n",
    "        columns={\n",
    "            'id':'product_id',\n",
    "            'name':'product_nme',\n",
    "            'code':'product_cde',\n",
    "            'description':'product_dsc',\n",
    "            'active_flag':'active_ind',\n",
    "            'owner_id.id':'owner_id',\n",
    "            'owner_id.name':'owner_nme',\n",
    "            'add_time':'insert_dte',\n",
    "            'update_time':'update_dte'\n",
    "        }, inplace=True)\n",
    "\n",
    "    # read individual price data\n",
    "    prices = pd.json_normalize(json_data, 'prices')[['id','price','product_id']]\n",
    "\n",
    "    # rename columns\n",
    "    prices.rename(\n",
    "        columns={\n",
    "            'id':'price_id',\n",
    "            'price':'price_amt'\n",
    "        }, inplace=True\n",
    "    )\n",
    "\n",
    "    # join price data back to product -- unnecessary complexity to keep both\n",
    "    products = products.join(prices.set_index('product_id'), on='product_id')\n",
    "\n",
    "    # convert to timestamps\n",
    "    for col in ['insert_dte','update_dte']:\n",
    "        products[col] = pd.to_datetime(products[col])\n",
    "        # conditionally strip timestamp\n",
    "        if dtm_to_dte:\n",
    "            products[col] = products[col].dt.date\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of products data from Pipedrive complete.\n",
      "Transform complete.\n",
      "execute_values() done\n",
      "Load to database complete.\n"
     ]
    }
   ],
   "source": [
    "etl('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'deals', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (2, 'organizations', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (4, 'products', 7, datetime.datetime(2020, 9, 17, 22, 46, 38, 13641)),\n",
       " (3, 'persons', 0, datetime.datetime(2020, 9, 17, 22, 52, 24, 590015))]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"select * from endpoint_tracker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  'Pro',\n",
       "  'PRO',\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2018, 12, 7),\n",
       "  datetime.date(2020, 6, 16),\n",
       "  1,\n",
       "  Decimal('3700.00')),\n",
       " (3,\n",
       "  'Inner Circle',\n",
       "  'IC',\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2019, 2, 5),\n",
       "  datetime.date(2019, 2, 5),\n",
       "  3,\n",
       "  Decimal('0.00')),\n",
       " (4,\n",
       "  'Dental Intel',\n",
       "  'DI',\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2019, 2, 5),\n",
       "  datetime.date(2019, 2, 5),\n",
       "  4,\n",
       "  Decimal('0.00')),\n",
       " (5,\n",
       "  'Connect',\n",
       "  'CONNECT',\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2019, 2, 5),\n",
       "  datetime.date(2019, 2, 5),\n",
       "  5,\n",
       "  Decimal('0.00')),\n",
       " (6,\n",
       "  'Academy',\n",
       "  'ACADEMY',\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2019, 2, 5),\n",
       "  datetime.date(2019, 2, 5),\n",
       "  6,\n",
       "  Decimal('0.00')),\n",
       " (8,\n",
       "  'Practice Assessment - Onsite',\n",
       "  None,\n",
       "  None,\n",
       "  True,\n",
       "  7316834,\n",
       "  'Kirk Behrendt',\n",
       "  datetime.date(2019, 6, 26),\n",
       "  datetime.date(2019, 6, 26),\n",
       "  8,\n",
       "  Decimal('0.00')),\n",
       " (9,\n",
       "  '5964',\n",
       "  None,\n",
       "  None,\n",
       "  True,\n",
       "  7573109,\n",
       "  'Barb James',\n",
       "  datetime.date(2019, 11, 21),\n",
       "  datetime.date(2019, 11, 21),\n",
       "  9,\n",
       "  Decimal('0.00'))]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"select * from products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_to_df(json_data, dtm_to_dte=True):\n",
    "    \n",
    "    persons = pd.json_normalize(json_data)[['id','first_name','last_name','open_deals_count','closed_deals_count','email_messages_count','activities_count','won_deals_count','lost_deals_count','active_flag','update_time','add_time','last_activity_date','last_incoming_mail_time','last_outgoing_mail_time','org_name','owner_id.id','owner_id.name','owner_id.active_flag','org_id.active_flag','org_id.value']]\n",
    "    \n",
    "    persons.rename(\n",
    "        columns={\n",
    "            'id':'persons_id',\n",
    "            'first_name':'first_nme',\n",
    "            'last_name':'last_nme',\n",
    "            'open_deals_count':'open_cnt',\n",
    "            'closed_deals_count':'closed_cnt',\n",
    "            'email_messages_count':'emails_cnt',\n",
    "            'activities_count':'activities_cnt',\n",
    "            'won_deals_count':'won_cnt',\n",
    "            'lost_deals_count':'lost_cnt',\n",
    "            'active_flag':'active_ind',\n",
    "            'update_time':'update_dte',\n",
    "            'add_time':'insert_dte',\n",
    "            'last_activity_date':'last_activity_dte',\n",
    "            'last_incoming_mail_time':'last_inmail_dte',\n",
    "            'last_outgoing_mail_time':'last_outmail_dte',\n",
    "            'org_name':'org_nme',\n",
    "            'owner_id.id':'owner_id',\n",
    "            'owner_id.name':'owner_nme',\n",
    "            'owner_id.active_flag':'owner_active_ind',\n",
    "            'org_id.active_flag':'org_active_ind',\n",
    "            'org_id.value':'org_id'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # convert to timestamps\n",
    "    for col in ['update_dte','insert_dte','last_activity_dte','last_inmail_dte','last_outmail_dte']:\n",
    "        persons[col] = pd.to_datetime(persons[col])\n",
    "        persons[col].fillna(pd.to_datetime('1900-01-01'), inplace=True)\n",
    "        # conditionally strip timestamp\n",
    "        if dtm_to_dte:\n",
    "            persons[col] = persons[col].dt.date\n",
    "        \n",
    "    # handle missing values\n",
    "    persons['org_nme'].fillna('', inplace=True)\n",
    "    persons['org_active_ind'].fillna(False, inplace=True)\n",
    "    persons['org_id'].fillna(-1, inplace=True)\n",
    "    \n",
    "    return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Areas for improvement\n",
    "\n",
    "Error handling on queries -- maybe return a specific number (-1) on failed query. Before moving on, check for -1.\n",
    "\n",
    "If we encounter -1 returned at ANY TIME, we should immediately break out of the ETL function.\n",
    "\n",
    "Should not increment position in endpoint_tracker if query or data pull is unsuccessful. Maybe push the increment later in the ETL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of persons data from Pipedrive complete.\n",
      "Transform complete.\n",
      "Error: duplicate key value violates unique constraint \"persons_pkey\"\n",
      "DETAIL:  Key (persons_id)=(359) already exists.\n",
      "\n",
      "Load to database complete.\n"
     ]
    }
   ],
   "source": [
    "etl('persons',limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate until we have all data\n",
    "while True:\n",
    "    etl('persons') # returns False if part of workflow fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16187,)]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, 'select max(persons_id) from persons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14908,\n",
       "  'Mark',\n",
       "  'Silberg',\n",
       "  2,\n",
       "  0,\n",
       "  83,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  True,\n",
       "  datetime.date(2020, 9, 17),\n",
       "  datetime.date(2019, 3, 29),\n",
       "  datetime.date(2020, 8, 21),\n",
       "  datetime.date(2020, 9, 17),\n",
       "  datetime.date(2020, 9, 17),\n",
       "  'Discovery Study Club',\n",
       "  7573109,\n",
       "  'Barb James',\n",
       "  True,\n",
       "  True,\n",
       "  459)]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, 'select * from persons where persons_id = 14908')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('persons_id',),\n",
       " ('first_nme',),\n",
       " ('last_nme',),\n",
       " ('open_cnt',),\n",
       " ('closed_cnt',),\n",
       " ('emails_cnt',),\n",
       " ('activities_cnt',),\n",
       " ('won_cnt',),\n",
       " ('lost_cnt',),\n",
       " ('active_ind',),\n",
       " ('update_dte',),\n",
       " ('insert_dte',),\n",
       " ('last_activity_dte',),\n",
       " ('last_inmail_dte',),\n",
       " ('last_outmail_dte',),\n",
       " ('org_nme',),\n",
       " ('owner_id',),\n",
       " ('owner_nme',),\n",
       " ('owner_active_ind',),\n",
       " ('org_active_ind',),\n",
       " ('org_id',)]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, \"select column_name from information_schema.columns where table_name = 'persons';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'deals', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (2, 'organizations', 0, datetime.datetime(2020, 9, 17, 22, 46, 21, 86157)),\n",
       " (4, 'products', 7, datetime.datetime(2020, 9, 17, 22, 46, 38, 13641)),\n",
       " (3, 'persons', 14682, datetime.datetime(2020, 9, 17, 22, 59, 4, 459471))]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(conn, 'select * from endpoint_tracker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deals_to_df(json_data, dtm_to_dte=True):\n",
    "    \n",
    "    deals = pd.json_normalize(json_data)[['id','stage_id','title','value','add_time','update_time','stage_change_time','active','status','lost_reason','close_time','pipeline_id','won_time','lost_time','products_count','files_count','notes_count','email_messages_count','activities_count','expected_close_date','last_incoming_mail_time','last_outgoing_mail_time','stage_order_nr','creator_user_id.id','user_id.id','person_id.value','org_id.value']]\n",
    "    \n",
    "    deals.rename(\n",
    "        columns={\n",
    "            'id':'deal_id',\n",
    "            'stage_id':'stage_id',\n",
    "            'title':'deal_nme',\n",
    "            'value':'deal_amt',\n",
    "            'add_time':'add_dte',\n",
    "            'update_time':'update_dte',\n",
    "            'stage_change_time':'stage_change_dte',\n",
    "            'active':'active_ind',\n",
    "            'status':'deal_status',\n",
    "            'lost_reason':'lost_dsc',\n",
    "            'close_time':'close_dte',\n",
    "            'pipeline_id':'pipeline_id',\n",
    "            'won_time':'won_dte',\n",
    "            'lost_time':'lost_dte',\n",
    "            'products_count':'product_cnt',\n",
    "            'files_count':'file_cnt',\n",
    "            'notes_count':'note_cnt',\n",
    "            'email_messages_count':'email_cnt',\n",
    "            'activities_count':'activity_cnt',\n",
    "            'expected_close_date':'expected_close_dte',\n",
    "            'last_incoming_mail_time':'last_inmail_dte',\n",
    "            'last_outgoing_mail_time':'last_outmail_dte',\n",
    "            'stage_order_nr':'stage_order_nbr',\n",
    "            'creator_user_id.id':'creator_id',\n",
    "            'user_id.id':'user_id',\n",
    "            'person_id.value':'person_id',\n",
    "            'org_id.value':'organization_id'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # convert to timestamps\n",
    "    for col in ['add_dte','update_dte','stage_change_dte','won_dte','lost_dte','expected_close_dte','last_inmail_dte','last_outmail_dte']:\n",
    "        deals[col] = pd.to_datetime(deals[col])\n",
    "        deals[col].fillna(pd.to_datetime('1900-01-01'), inplace=True)\n",
    "        # conditionally strip timestamp\n",
    "        if dtm_to_dte:\n",
    "            deals[col] = deals[col].dt.date\n",
    "        \n",
    "    # handle missing values\n",
    "    deals = fill_missing(deals)\n",
    "    \n",
    "    return deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of deals data from Pipedrive complete.\n"
     ]
    }
   ],
   "source": [
    "deals, deals_start = get_data('deals',start=0,limit=100)\n",
    "deals = deals_to_df(deals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

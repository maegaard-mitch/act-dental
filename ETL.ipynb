{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "import database as db # our script for simpler db integration\n",
    "from config import config # pull sensitive data from .ini files\n",
    "\n",
    "# set data directory\n",
    "data_path = (os.getcwd() + \"/Data/\")\n",
    "script_path = (os.getcwd() + \"/SQL/\")\n",
    "\n",
    "# connection to Postgres database\n",
    "conn = db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_KEEPS = {\n",
    "    'deals':['id','creator_user_id.id','user_id.id','org_id.value','stage_id','title','value','stage_change_time','status','won_time','lost_reason','email_messages_count','activities_count','active','add_time','update_time'],\n",
    "    'organizations':['id','name','owner_id.id','address','active_flag','add_time','update_time'],\n",
    "    'persons':['id','owner_id.id','org_id','first_name','last_name','active_flag','add_time','update_time'],\n",
    "    'pipelines':['id','name','order_nr','active','add_time','update_time'],\n",
    "    'stages':['id','order_nr','name','pipeline_id','rotten_flag','rotten_days','active_flag','add_time','update_time'],\n",
    "    'users':['id','name','email','active_flag','created','modified']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create tables for database\n",
    "\n",
    "See details in Schema folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract data from Pipedrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(endpoint, start, limit):\n",
    "    \n",
    "    params = config(section='pipedrive')\n",
    "    params.update({'start':start,'limit':limit})\n",
    "    \n",
    "    response = requests.get(params['company_domain'] + endpoint, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        json_msg = response.json()\n",
    "        print('Extraction of {} data from Pipedrive complete.'.format(endpoint))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return\n",
    "    \n",
    "    data = json_msg['data']\n",
    "    \n",
    "    if endpoint in ['users','stages']:\n",
    "        next_start = len(data)\n",
    "    else:\n",
    "        if json_msg['additional_data']['pagination']['more_items_in_collection']:\n",
    "            next_start = json_msg['additional_data']['pagination']['next_start']\n",
    "        elif data is None:\n",
    "            next_start = start\n",
    "        else:\n",
    "            next_start = len(data)\n",
    "    \n",
    "    return data, next_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(data):\n",
    "    \n",
    "    date_cols = [col for col in data.columns if '_dte' in col]\n",
    "    \n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col]).dt.date\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(data):\n",
    "    \"\"\"\n",
    "    Following rulesets should be applied for filling missing data\n",
    "    \n",
    "    ID's : -1\n",
    "    Names : blank ('')\n",
    "    Descriptions : blank ('')\n",
    "    Timestamps: 1900-01-01\n",
    "    Counts/amounts/numbers: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    id_cols = [col for col in data.columns if '_id' in col]\n",
    "    nme_cols = [col for col in data.columns if '_nme' in col]\n",
    "    dsc_cols = [col for col in data.columns if '_dsc' in col]\n",
    "    cnt_cols = [col for col in data.columns if '_cnt' in col]\n",
    "    amt_cols = [col for col in data.columns if '_amt' in col]\n",
    "    nbr_cols = [col for col in data.columns if '_nbr' in col]\n",
    "    dte_cols = [col for col in data.columns if '_dte' in col]\n",
    "    \n",
    "    for col in (id_cols + cnt_cols + amt_cols + nbr_cols):\n",
    "        data[col].fillna(-1, inplace=True)\n",
    "    \n",
    "    for col in (nme_cols + dsc_cols):\n",
    "        data[col].fillna('', inplace=True)\n",
    "    \n",
    "    for col in (dte_cols):\n",
    "        data[col].fillna('1900-01-01', inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(json_data, json_keeps, df_rename):\n",
    "    \n",
    "    data = pd.json_normalize(json_data)[json_keeps]\n",
    "    \n",
    "    data.rename(columns=dict(zip(json_keeps, df_rename)), inplace=True)\n",
    "    \n",
    "    data = to_date(data)\n",
    "    data = fill_missing(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(endpoint, limit=500):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        endpoint - Pipedrive endpoint to retreive data from\n",
    "        limit - the amount of data to retrieve in this batch\n",
    "    \n",
    "    returns output message for failure/success\n",
    "    \"\"\"\n",
    "    \n",
    "    if endpoint not in [*JSON_KEEPS]:\n",
    "        print('{} is not a valid endpoint. Please try again.'.format(endpoint))\n",
    "        return\n",
    "    \n",
    "    start = db.execute_query(conn, \"select start_nbr from endpoints where endpoint_nme = '{}';\".format(endpoint))\n",
    "    \n",
    "    if start == -1:\n",
    "        return\n",
    "    start = start[0][0] # unpack tuple\n",
    "    \n",
    "    # read max rows from most recently stored data\n",
    "    json_data, next_start = get_data(endpoint, start=start, limit=limit)\n",
    "    \n",
    "    if start == next_start:\n",
    "        print('No new data. Check back later.')\n",
    "        return\n",
    "    \n",
    "    if json_data is not None:\n",
    "        \n",
    "        table_cols = db.execute_query(conn, \"select column_name from information_schema.columns where table_name = '{}';\".format(endpoint))\n",
    "        name_updates = [col_name for i in table_cols for col_name in i]\n",
    "        data = json_to_df(\n",
    "            json_data, # json data\n",
    "            JSON_KEEPS[endpoint], # data names from json\n",
    "            name_updates # new column names for df\n",
    "        )\n",
    "        \n",
    "        # write data to database (will rollback if not successful)\n",
    "        if endpoint == 'users':\n",
    "            table = 'employees'\n",
    "        else:\n",
    "            table = endpoint\n",
    "        write_data = db.execute_values(conn, data, table)\n",
    "        \n",
    "        if write_data == -1:\n",
    "            return\n",
    "        \n",
    "        # increment tracker to new start\n",
    "        update_endpoint = db.execute_query(conn, \"update endpoints set start_nbr = {pos}, update_dtm = current_timestamp where endpoint_nme = '{endpoint}'\".format(pos=next_start, endpoint=endpoint))\n",
    "        \n",
    "        if update_endpoint == -1:\n",
    "            return\n",
    "\n",
    "        print('Load to database complete.')\n",
    "        \n",
    "    else:\n",
    "        print('No more {} data - tracker at position {}.'.format(endpoint, start))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of users data from Pipedrive complete.\n",
      "No new data. Check back later.\n"
     ]
    }
   ],
   "source": [
    "etl('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of pipelines data from Pipedrive complete.\n",
      "Load to database complete.\n"
     ]
    }
   ],
   "source": [
    "etl('pipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of pipelines data from Pipedrive complete.\n",
      "No new data. Check back later.\n"
     ]
    }
   ],
   "source": [
    "etl('pipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of stages data from Pipedrive complete.\n",
      "No new data. Check back later.\n"
     ]
    }
   ],
   "source": [
    "etl('stages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction of organizations data from Pipedrive complete.\n",
      "Load to database complete.\n"
     ]
    }
   ],
   "source": [
    "etl('organizations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

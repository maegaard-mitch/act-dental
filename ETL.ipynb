{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "import database as db # our script for simpler db integration\n",
    "from config import config # pull sensitive data from .ini files\n",
    "\n",
    "# set data directory\n",
    "data_path = (os.getcwd() + \"/Data/\")\n",
    "\n",
    "# connection to Postgres database\n",
    "conn = db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_KEEPS = {\n",
    "    'deals':['id','creator_user_id.id','user_id.id','org_id.value','stage_id','title','value','stage_change_time','status','won_time','lost_reason','email_messages_count','activities_count','active','add_time','update_time'],\n",
    "    'organizations':['id','name','owner_id.id','address','active_flag','add_time','update_time'],\n",
    "    'persons':['id','owner_id.id','org_id','first_name','last_name','active_flag','add_time','update_time'],\n",
    "    'pipelines':['id','name','order_nr','active','add_time','update_time'],\n",
    "    'stages':['id','order_nr','name','pipeline_id','rotten_flag','rotten_days','active_flag','add_time','update_time'],\n",
    "    'users':['id','name','email','active_flag','created','modified']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create tables for database\n",
    "\n",
    "See details in Schema folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract data from Pipedrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(endpoint, start, limit):\n",
    "    \n",
    "    params = config(section='pipedrive')\n",
    "    params.update({'start':start,'limit':limit})\n",
    "    \n",
    "    response = requests.get(params['company_domain'] + endpoint, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        json_msg = response.json()\n",
    "        #print('Extraction of {} data from Pipedrive complete.'.format(endpoint))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return\n",
    "    \n",
    "    data = json_msg['data']\n",
    "    \n",
    "    if endpoint not in ['deals','organizations','persons','pipelines']:\n",
    "        next_start = len(data)\n",
    "    else:\n",
    "        if json_msg['additional_data']['pagination']['more_items_in_collection']:\n",
    "            next_start = json_msg['additional_data']['pagination']['next_start']\n",
    "        elif data is None:\n",
    "            next_start = start\n",
    "        else:\n",
    "            next_start = start + len(data)\n",
    "    \n",
    "    return data, next_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(data):\n",
    "    \n",
    "    date_cols = [col for col in data.columns if '_dte' in col]\n",
    "    \n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col]).dt.date\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(data):\n",
    "    \"\"\"\n",
    "    Following rulesets should be applied for filling missing data\n",
    "    \n",
    "    ID's : -1\n",
    "    Names : blank ('')\n",
    "    Descriptions : blank ('')\n",
    "    Timestamps: 1900-01-01\n",
    "    Counts/amounts/numbers: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    id_cols = [col for col in data.columns if '_id' in col]\n",
    "    nme_cols = [col for col in data.columns if '_nme' in col]\n",
    "    dsc_cols = [col for col in data.columns if '_dsc' in col]\n",
    "    cnt_cols = [col for col in data.columns if '_cnt' in col]\n",
    "    amt_cols = [col for col in data.columns if '_amt' in col]\n",
    "    nbr_cols = [col for col in data.columns if '_nbr' in col]\n",
    "    dte_cols = [col for col in data.columns if '_dte' in col]\n",
    "    addr_cols = [col for col in data.columns if '_addr' in col]\n",
    "    \n",
    "    for col in (id_cols + cnt_cols + amt_cols + nbr_cols):\n",
    "        data[col].fillna(-1, inplace=True)\n",
    "    \n",
    "    for col in (nme_cols + dsc_cols + addr_cols):\n",
    "        data[col].fillna('', inplace=True)\n",
    "    \n",
    "    for col in (dte_cols):\n",
    "        data[col].fillna('1900-01-01', inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(json_data, json_keeps, df_rename):\n",
    "    \n",
    "    data = pd.json_normalize(json_data)[json_keeps]\n",
    "    \n",
    "    data.rename(columns=dict(zip(json_keeps, df_rename)), inplace=True)\n",
    "    \n",
    "    data = to_date(data)\n",
    "    data = fill_missing(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(endpoint, limit=500):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        endpoint - Pipedrive endpoint to retreive data from\n",
    "        limit - the amount of data to retrieve in this batch\n",
    "    \n",
    "    returns output message for failure/success\n",
    "    \"\"\"\n",
    "    \n",
    "    if endpoint not in [*JSON_KEEPS]:\n",
    "        print('{} is not a valid endpoint. Please try again.'.format(endpoint))\n",
    "        return False\n",
    "    \n",
    "    start = db.execute_query(conn, \"select start_nbr from endpoints where endpoint_nme = '{}';\".format(endpoint))\n",
    "    \n",
    "    if start == -1: return False\n",
    "    start = start[0][0] # unpack tuple\n",
    "    \n",
    "    # read max rows from most recently stored data\n",
    "    json_data, next_start = get_data(endpoint, start=start, limit=limit)\n",
    "    \n",
    "    if (start == next_start) or (json_data is None):\n",
    "        print('No new data. Check back later.')\n",
    "        return False\n",
    "        \n",
    "    table_cols = db.execute_query(conn, \"select column_name from information_schema.columns where table_name = '{}';\".format(endpoint))\n",
    "    name_updates = [col_name for i in table_cols for col_name in i]\n",
    "    data = json_to_df(\n",
    "        json_data, # json data\n",
    "        JSON_KEEPS[endpoint], # data names from json\n",
    "        name_updates # new column names for df\n",
    "    )\n",
    "\n",
    "    # write data to database (will rollback if not successful)\n",
    "    if endpoint == 'users': table = 'employees'\n",
    "    else: table = endpoint\n",
    "        \n",
    "    write_data = db.execute_values(conn, data, table)\n",
    "\n",
    "    if write_data == -1: return False\n",
    "\n",
    "    # increment tracker to new start\n",
    "    update_endpoint = db.execute_query(conn, \"update endpoints set start_nbr = {pos}, update_dtm = current_timestamp where endpoint_nme = '{endpoint}'\".format(pos=next_start, endpoint=endpoint))\n",
    "\n",
    "    if update_endpoint == -1: return False\n",
    "    \n",
    "    print('Load to database complete.')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data(endpoint):\n",
    "    \n",
    "    table_cols = db.execute_query(conn, \"select column_name from information_schema.columns where table_name = '{}';\".format(endpoint))\n",
    "    name_updates = [col_name for i in table_cols for col_name in i]\n",
    "    \n",
    "    ids = db.execute_query(conn,\"\"\"\n",
    "        SELECT all_ids AS missing_ids\n",
    "        FROM generate_series(1, (SELECT MAX({id}) FROM {endpoint})) all_ids\n",
    "        EXCEPT \n",
    "        SELECT {id} FROM {endpoint}\n",
    "        order by missing_ids;\n",
    "    \"\"\".format(endpoint=endpoint, id=table_cols[0][0]))\n",
    "    \n",
    "    if not ids:\n",
    "        print('No additional data from {}'.format(endpoint))\n",
    "        return False\n",
    "    \n",
    "    # initialize columns\n",
    "    data = pd.DataFrame(columns=name_updates)\n",
    "    \n",
    "    for i in ids:\n",
    "        data = data.append(json_to_df(\n",
    "            get_data('{endpoint}/{id}'.format(endpoint=endpoint, id=i[0]),0,10)[0],\n",
    "            JSON_KEEPS[endpoint],\n",
    "            name_updates\n",
    "        ), ignore_index=True)\n",
    "    \n",
    "    write_data = db.execute_values(conn, data, endpoint)\n",
    "    \n",
    "    if write_data == -1: return False\n",
    "    \n",
    "    print('Loaded missing {} data to database complete.'.format(endpoint))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint in [*JSON_KEEPS]:\n",
    "    \n",
    "    # initialize looping condition\n",
    "    continue_load = True\n",
    "    \n",
    "    while continue_load:\n",
    "        # API -> Postgres until no new data\n",
    "        continue_load = etl(endpoint)\n",
    "    \n",
    "    # get id's that may have been skipped (deleted, inactive, etc.)\n",
    "    get_missing_data(endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
